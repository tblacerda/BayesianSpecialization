params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
library("car")  # load the 'car' package
install.packages("car")
library("car")  # load the 'car' package
library("car")  # load the 'car' package
install.packages("car")
data("PlantGrowth")
?PlantGrowth
head(PlantGrowth)
str(PlantGrowth)
boxplot(weight ~ group, data = PlantGrowth,
xlab = "Group", ylab = "Weight",
main = "Boxplot of Weight by Group",
col = c("red", "green", "blue"))
lmod = lm(weight ~ group, data = PlantGrowth)
summary(lmod)
anova(lmod)
mod_string = "model {
for (i in 1:lenght(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0, 1/1.0e6)
}
prec ~ dgamma(5/2.0, 5*1.0/2)
sig = sqrt (1 / prec)
}"
set.seed(82)
library(rjags)
data_jags = list(y = PlantGrowth$weight,
grp = as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
list("mu"=rnorm(3, 0, 100), "prec" = rgamma(1, 1, 1))
}
mod = jags.model(textConnection(mod_string),
data = data_jags,
inits = inits,
n.chains = 3,
n.adapt = 1000
)
mod_string = "model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0, 1/1.0e6)
}
prec ~ dgamma(5/2.0, 5*1.0/2)
sig = sqrt (1 / prec)
}"
set.seed(82)
library(rjags)
data_jags = list(y = PlantGrowth$weight,
grp = as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
list("mu"=rnorm(3, 0, 100), "prec" = rgamma(1, 1, 1))
}
mod = jags.model(textConnection(mod_string),
data = data_jags,
inits = inits,
n.chains = 3,
n.adapt = 1000
)
update(mod, 1000)
mod_sim = coda.samples(model = mod,
variable.names = params,
n.iter = 10000,
thin = 10
)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
mod_csim
mod_sim
mod_sim[0]
plot(mod_sim)
plot(mod_csim)
plot(mod_sim)
?coda_samples
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
effectiveSize(mod_sim)
c("mu[1]", "mu[2]", "mu[3]", "sig"
pm_params(colMeans(mod_csim),
pm_params = (colMeans(mod_csim))
pm_params
yhat = pm_params[1:3][data_jags$grp]
data_jags$grp
yhat
resid = data_jags$y - yhat
resid
plot(resid ~ yhat,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
plot(resid)
plot(resid ~ yhat,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
summary(mod_sim)
HPDinterval(mod_sim, prob = 0.95)
HPDinterval(mod_csim, prob = 0.95)
mod_csim[,3] > mod_csim[,1]
mean(mod_csim[,3] > mod_csim[,1])
mean(mod_csim[,3] > 1.1* mod_csim[,1])
data("PlantGrowth")
?PlantGrowth
head(PlantGrowth)
str(PlantGrowth)
boxplot(weight ~ group, data = PlantGrowth,
xlab = "Group", ylab = "Weight",
main = "Boxplot of Weight by Group",
col = c("red", "green", "blue"))
lmod = lm(weight ~ group, data = PlantGrowth)
summary(lmod)
anova(lmod)
mod_string = "model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0, 1/1.0e6)
}
prec ~ dgamma(5/2.0, 5*1.0/2)
sig = sqrt (1 / prec)
}"
set.seed(82)
library(rjags)
data_jags = list(y = PlantGrowth$weight,
grp = as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
list("mu"=rnorm(3, 0, 100), "prec" = rgamma(1, 1, 1))
}
mod = jags.model(textConnection(mod_string),
data = data_jags,
inits = inits,
n.chains = 3,
n.adapt = 1000
)
update(mod, 1000)
mod_sim = coda.samples(model = mod,
variable.names = params,
n.iter = 10000,
thin = 10
)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(mod_sim)
install.packages("data.table")
install.packages("plotly")
library(SimDesign)
?Simdesign
?SimDesign
?bias
library(datasauRus)
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap(~dataset,ncol=3)
library(tidyverse)
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap(~dataset,ncol=3)
library(tidyverse)
install.packages("tidyverse")
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap(~dataset,ncol=3)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
data("datasaurus_dozen")
str(datasaurus_dozen)
head(datasaurus_dozen)
select(datasaurus_dozen)
install.packages("tidyverse")
library(tidyverse)
install.packages(rethinking)
install.packages("rethinking")
install.packages(c("coda","mvtnorm","devtools"))
q()
library(readxl)
library(dplyr)
library(rjags)
library(coda)
setwd("~/Documentos/COURSERA_BAYES_ESPECIALIZATION/Course 2 - Tecniques and models/Applications/MarkovChainMonteCarlo_ECQ")
data <- read_excel("DADOS BRUTOS/ECQ_ABR_25.xlsx", sheet = "Export") %>%
head(-2) %>%
select(ANF, MUNICIPIO, TESTES_ECQ, ECQ) %>%
mutate(
TESTES_ECQ_OK = round(TESTES_ECQ * ECQ, 0)
) %>%
group_by(ANF, MUNICIPIO) %>%
summarise(
TESTES_ECQ = sum(TESTES_ECQ, na.rm = TRUE),
TESTES_ECQ_OK = sum(TESTES_ECQ_OK, na.rm = TRUE),
.groups = "drop"
) %>%
group_by(ANF, MUNICIPIO) %>%
mutate(group_id = cur_group_id()) %>%
ungroup()
View(data)
# Dados para JAGS
jags_data <- list(
N_group = max(data$group_id),        # Número de grupos únicos
N_obs = nrow(data),                  # Número de observações
group_id = data$group_id,            # ID do grupo para cada observação
n_tests = data$TESTES_ECQ,           # Número de testes por observação
n_success = data$TESTES_ECQ_OK       # Sucessos por observação
)
# Modelo JAGS corrigido incluindo a verossimilhança
model_string <- "
model {
# Hiperparâmetros globais
mu_global ~ dbeta(5, 3)
sigma_global ~ dgamma(2, 0.5)
# Parâmetros ANF
alpha_anf <- mu_global * sigma_global
beta_anf <- (1 - mu_global) * sigma_global
mu_anf ~ dbeta(alpha_anf, beta_anf)
# Dispersão para municípios
phi_municipio ~ dgamma(2, 0.9)
# Hierarquia por município
for(g in 1:N_group) {
a_municipio[g] <- mu_anf * phi_municipio
b_municipio[g] <- (1 - mu_anf) * phi_municipio
mu_municipio[g] ~ dbeta(a_municipio[g], b_municipio[g])
}
# Verossimilhança (Binomial)
for(i in 1:N_obs) {
n_success[i] ~ dbin(mu_municipio[group_id[i]], n_tests[i])
}
}
"
# Função de inicialização
inits <- function() {
list(
mu_global = rbeta(1, 5, 3),
sigma_global = rgamma(1, 2, 0.5),
phi_municipio = rgamma(1, 2, 0.9),
mu_anf = rbeta(1, 3, 3)
)
}
setwd("~/Documentos/COURSERA_BAYES_ESPECIALIZATION/Course 2 - Tecniques and models/Applications/MarkovChainMonteCarlo_ECQ")
# Configurar e executar o modelo
model <- jags.model(
textConnection(model_string),
data = jags_data,
inits = inits,
n.chains = 4,
n.adapt = 1000
)
samples <- coda.samples(
model,
variable.names = c("mu_anf", "mu_municipio"),
n.iter = 5000,
thin = 2
)
# Verificar convergência
gelman.diag(samples)
group_map <- data %>%
distinct(group_id, ANF, MUNICIPIO)
municipio_stats <- samples %>%
spread_draws(mu_municipio[group_id]) %>%
group_by(group_id) %>%
summarise(
mean_municipio = mean(mu_municipio),
stddev_municipio = sd(mu_municipio),
hdi_inf_Mun = hdi(mu_municipio, credMass = 0.95)[1],
hdi_sup_Mun = hdi(mu_municipio, credMass = 0.95)[2]
) %>%
left_join(group_map, by = "group_id")
# 2. Extrair estatísticas dos municípios
municipio_stats <- samples %>%
spread_draws(mu_municipio[group_id]) %>%
group_by(group_id) %>%
summarise(
mean_municipio = mean(mu_municipio),
stddev_municipio = sd(mu_municipio),
hdi_inf_Mun = hdi(mu_municipio, credMass = 0.95)[1],
hdi_sup_Mun = hdi(mu_municipio, credMass = 0.95)[2]
) %>%
left_join(group_map, by = "group_id")
library(tidybayes)
# 2. Extrair estatísticas dos municípios
municipio_stats <- samples %>%
spread_draws(mu_municipio[group_id]) %>%
group_by(group_id) %>%
summarise(
mean_municipio = mean(mu_municipio),
stddev_municipio = sd(mu_municipio),
hdi_inf_Mun = hdi(mu_municipio, credMass = 0.95)[1],
hdi_sup_Mun = hdi(mu_municipio, credMass = 0.95)[2]
) %>%
left_join(group_map, by = "group_id")
municipio_stats
View(data)
# 2. Extrair estatísticas dos municípios
municipio_stats <- samples %>%
spread_draws(mu_municipio[group_id]) %>%
group_by(group_id) %>%
summarise(
mean_municipio = mean(mu_municipio),
stddev_municipio = sd(mu_municipio),
hdi_inf_Mun = hdi(mu_municipio, credMass = 0.95)[1],
hdi_sup_Mun = hdi(mu_municipio, credMass = 0.95)[2]
) %>%
left_join(group_map, by = "group_id")
municipio_stats <- samples %>%
spread_draws(mu_municipio[group_id]) %>%
group_by(group_id) %>%
summarise(
mean_municipio = mean(mu_municipio),
stddev_municipio = sd(mu_municipio),
hdi_inf_Mun = hdi(mu_municipio, credMass = 0.95, na.rm = TRUE)[1],  # <- Fix here
hdi_sup_Mun = hdi(mu_municipio, credMass = 0.95, na.rm = TRUE)[2]   # <- And here
) %>%
left_join(group_map, by = "group_id")
mcmc_areas(samples, pars = "mu_municipio[128]")  # Replace 128 with the problematic group
mcmc(samples, pars = "mu_municipio[128]")  # Replace 128 with the problematic group
