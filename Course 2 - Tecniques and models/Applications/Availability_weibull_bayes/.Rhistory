ungroup() %>%
mutate(
Cell  = fct_reorder(Cell, mediana_acc),
Grupo = as.factor(Grupo)
) %>%
ggplot(aes(x = Cell, y = acc, color = Grupo)) +
geom_jitter(width  = 0.2,
height = 0,
size   = 1.5,
alpha  = 0.6) +
scale_color_manual(
name   = "Grupo",
values = c("COM AUTOMAÇÃO" = "blue",
"SEM AUTOMAÇÃO" = "green")
) +
theme(
axis.text.x = element_text(angle = 90, hjust = 1)
) +
xlab("Cell") +
ylab("Acc") +
ggtitle("Distribuição de Acc por Cell, separado por Grupo")
setwd("~/Documentos/COURSERA_BAYES_ESPECIALIZATION/CURSO 2 - Tecniques and models/disponibilidade_weibull_bayes")
library(tidyverse)
library(lubridate)
library(rjags)
library(readxl)
LIMITE <- 22/24
ANFS <- c(83)
# Read and prepare data
data <- read_excel("data.xlsx") %>%
select(-any_of("index")) %>%
rename(Data = 1, Disp = 2, Site = 3, dia = 4) %>%
mutate(Data = as_datetime(Data),
dia = floor_date(Data, "day"))
data
data <- read_excel("data.xlsx")
str(data)
data %>%   select(-any_of("index"))
data %>% select(-any_of("index"))
data %>% select(-index)
head(data)
data %>% select(-index)
data <- read_excel("data.xlsx")
data %>% select(-index)
head(data)
data %>% select(-index)
library(tidyverse)
library(lubridate)
library(rjags)
library(readxl)
LIMITE <- 22/24
ANFS <- c(83)
data <- read_excel("data.xlsx")
head(data)
data %>% select(-index)
head(data)
library(dplyr)
data %>% select(-index)
data %>% dplyr::select(-index)
data <- read_excel("data.xlsx") %>%
dplyr::select(-index, -dia)
data
data <- read_excel("data.xlsx") %>%
dplyr::select(-index, -dia) %>%
mutate(Data = as_datetime(Data)
data <- read_excel("data.xlsx") %>%
data <- read_excel("data.xlsx") %>%
dplyr::select(-index, -dia) %>%
mutate(Data = as_datetime(Data))
data
str(data)
# Create daily events summary
daily <- data %>%
group_by(Site, Data) %>%
summarize(evento = as.integer(any(Disp <= LIMITE)), .groups = "drop") %>%
rename(equipamento_id = Site, periodo = Data) %>%
mutate(periodo = as_datetime(periodo))
daily
sum(daily$evento)
# Bayesian Weibull model function
bayesian_weibull_model <- function(durations, events) {
model_string <- "
model {
# Priors
lambda ~ dgamma(2, 1)
rho ~ dgamma(2, 1)
# Likelihood using zeros trick for custom distribution
for (i in 1:N) {
log_pdf <- log(rho) + (rho-1)*log(durations[i]) - rho*log(lambda) - pow(durations[i]/lambda, rho)
log_S <- -pow(durations[i]/lambda, rho)
log_lik[i] <- events[i]*log_pdf + (1-events[i])*log_S
zeros[i] <- 0
zeros[i] ~ dpois(-log_lik[i] + C)
}
C <- 1000000  # Large constant to avoid negative values
}
"
jags_data <- list(
durations = durations,
events = events,
N = length(durations),
zeros = rep(0, length(durations))
)
inits <- function() {
list(
lambda = runif(1, 0.1, 10),
rho = runif(1, 0.1, 10)
)
}
model <- jags.model(textConnection(model_string),
data = jags_data,
inits = inits,
n.chains = 2,
quiet = TRUE)
update(model, 500, progress.bar = "none")
samples <- coda.samples(model,
variable.names = c("lambda", "rho"),
n.iter = 1000,
progress.bar = "none")
return(samples)
}
# Process sites
probs_bayesian <- list()
pb <- progress::progress_bar$new(total = length(unique(daily$equipamento_id)),
format = "Processing sites [:bar] :percent")
for (site in unique(daily$equipamento_id)) {
pb$tick()
tryCatch({
df_site <- daily %>%
filter(equipamento_id == site) %>%
arrange(periodo)
event_dates <- df_site %>% filter(evento == 1) %>% pull(periodo)
all_dates <- df_site %>% pull(periodo)
durations <- numeric()
events <- integer()
prev_date <- NULL
for (date in event_dates) {
if (is.null(prev_date)) {
start_date <- min(all_dates)
duration <- as.numeric(difftime(date, start_date, units = "days"))
} else {
duration <- as.numeric(difftime(date, prev_date, units = "days"))
}
durations <- c(durations, duration)
events <- c(events, 1L)
prev_date <- date
}
if (!is.null(prev_date)) {
end_date <- max(all_dates)
duration <- as.numeric(difftime(end_date, prev_date, units = "days"))
durations <- c(durations, duration)
events <- c(events, 0L)
}
if (length(durations) < 2) {
probs_bayesian[[as.character(site)]] <- NA_real_
next
}
samples <- bayesian_weibull_model(durations, events)
samples_matrix <- as.matrix(samples)
s <- tail(durations, 1)
lambda_samples <- samples_matrix[, "lambda"]
rho_samples <- samples_matrix[, "rho"]
S_s <- exp(-(s / lambda_samples)^rho_samples)
S_s_plus_1 <- exp(-((s + 1) / lambda_samples)^rho_samples)
prob_samples <- (S_s - S_s_plus_1) / S_s
probs_bayesian[[as.character(site)]] <- mean(prob_samples)
}, error = function(e) {
message(sprintf("Error in site %s: %s", site, conditionMessage(e)))
probs_bayesian[[as.character(site)]] <<- NA_real_
})
}
probs_df <- tibble(
Site = names(probs_bayesian),
Probability = unlist(probs_bayesian)
) %>%
arrange(desc(Probability)) %>%
filter(Probability > 0.05)
probs_df
library(jsonlite)
data <- fromJSON("DISP4G.json")
data <- data %>% select(Dia, ANF, Site, Disp)
data
data <- data %>% filter(ANF == 83)
data
data <- data %>%
filter(ANF == 83) %>%
mutate(
Data = as_datetime(Dia),
) %>%
select(-Dia)
data <- data %>%
filter(ANF == 83) %>%
mutate(Data = as_datetime(Dia)) %>%
select(-Dia)
data <- data %>%
filter(ANF == 83) %>%
mutate(Data = as_datetime(Dia)) %>%
dplyr::select(-Dia)
data
data %>% floor_date(Data, "day")
head(data)
data %>% floor_date(Data, "day")
?floor_date
library(tidyverse)
library(lubridate)
library(rjags)
library(readxl)
library(jsonlite)
LIMITE <- 22/24
data <- fromJSON("DISP4G.json")
data <- data %>%
filter(ANF == 83) %>%
mutate(dia = as_datetime(Dia)) %>%
dplyr::select(-Dia)
head(data)
daily <- data %>%
group_by(Site, dia) %>%
summarize(evento = as.integer(any(Disp <= LIMITE)), .groups = "drop") %>%
rename(periodo = dia) %>%
mutate(periodo = as_datetime(periodo))
daily
bayesian_weibull_model <- function(durations, events) {
model_string <- "
model {
# Priors (same as Python)
lambda ~ dgamma(2, 1)
rho ~ dgamma(2, 1)
# Custom likelihood (direct translation of Python Potential)
for (i in 1:N) {
log_pdf <- log(rho) + (rho-1)*log(durations[i]) - rho*log(lambda) - pow(durations[i]/lambda, rho)
log_S <- -pow(durations[i]/lambda, rho)
log_lik[i] <- events[i]*log_pdf + (1-events[i])*log_S
# Ones trick equivalent to Python's Potential
ones[i] ~ dbern(exp(log_lik[i]))
}
}
"
jags_data <- list(
durations = durations,
events = events,
N = length(durations),
ones = rep(1, length(durations))
)
inits <- function() {
list(
lambda = runif(1, 0.1, 10),
rho = runif(1, 0.1, 10)
)
}
model <- jags.model(textConnection(model_string),
data = jags_data,
inits = inits,
n.chains = 2,
quiet = TRUE)
update(model, 500)
samples <- coda.samples(model,
variable.names = c("lambda", "rho"),
n.iter = 1000)
return(samples)
}
process_site <- function(df) {
event_dates <- df %>% filter(evento == 1) %>% pull(periodo)
all_dates <- df %>% pull(periodo)
if (length(event_dates) == 0) return(NA_real_)
durations <- diff(sort(c(min(all_dates), event_dates, max(all_dates)))) %>%
as.numeric(units = "days")
events <- c(rep(1, length(event_dates)), 0)
if (length(durations) < 2) return(NA_real_)
samples <- bayesian_weibull_model(durations, events)
samples_matrix <- as.matrix(samples)
s <- tail(durations, 1)
lambda <- samples_matrix[, "lambda"]
rho <- samples_matrix[, "rho"]
mean(exp(-(s/lambda)^rho - ((s+1)/lambda)^rho))
}
probs_df <- daily %>%
group_by(Site) %>%
group_modify(~ {
result <- tryCatch(
list(Probability = process_site(.x)),
error = function(e) list(Probability = NA_real_))
tibble(Probability = result$Probability)
}) %>%
filter(!is.na(Probability), Probability > 0.05) %>%
arrange(desc(Probability))
probs_df
# Bayesian Weibull model matching PyMC3 version
bayesian_weibull_model <- function(durations, events) {
model_string <- "
model {
# Priors (same as Python)
lambda ~ dgamma(2, 1)
rho ~ dgamma(2, 1)
# Custom likelihood (direct translation of Python Potential)
for (i in 1:N) {
log_pdf <- log(rho) + (rho-1)*log(durations[i]) - rho*log(lambda) - pow(durations[i]/lambda, rho)
log_S <- -pow(durations[i]/lambda, rho)
log_lik[i] <- events[i]*log_pdf + (1-events[i])*log_S
# Ones trick equivalent to Python's Potential
ones[i] ~ dbern(exp(log_lik[i]))
}
}
"
jags_data <- list(
durations = durations,
events = events,
N = length(durations),
ones = rep(1, length(durations))
)
inits <- function() {
list(
lambda = runif(1, 0.1, 10),
rho = runif(1, 0.1, 10)
)
}
model <- jags.model(textConnection(model_string),
data = jags_data,
inits = inits,
n.chains = 2,
quiet = TRUE)
update(model, 500)
samples <- coda.samples(model,
variable.names = c("lambda", "rho"),
n.iter = 1000)
return(samples)
}
# Processing function matching Python logic
process_site <- function(df) {
event_dates <- df %>% filter(evento == 1) %>% pull(periodo)
all_dates <- df %>% pull(periodo)
if (length(event_dates) == 0) return(NA_real_)
durations <- diff(sort(c(min(all_dates), event_dates, max(all_dates)))) %>%
as.numeric(units = "days")
events <- c(rep(1, length(event_dates)), 0)
if (length(durations) < 2) return(NA_real_)
samples <- bayesian_weibull_model(durations, events)
samples_matrix <- as.matrix(samples)
s <- tail(durations, 1)
lambda <- samples_matrix[, "lambda"]
rho <- samples_matrix[, "rho"]
mean(exp(-(s/lambda)^rho - ((s+1)/lambda)^rho))
}
# Final processing
probs_df <- daily %>%
group_by(Site) %>%
group_modify(~ {
result <- tryCatch(
list(Probability = process_site(.x)),
error = function(e) list(Probability = NA_real_))
tibble(Probability = result$Probability)
}) %>%
filter(!is.na(Probability), Probability > 0.05) %>%
arrange(desc(Probability))
probs_df
library(tidyverse)
library(lubridate)
library(rjags)
library(readxl)
library(jsonlite)
LIMITE <- 22/24
data <- fromJSON("DISP4G.json")
data <- data %>%
filter(ANF == 83) %>%
mutate(dia = as_datetime(Dia)) %>%
dplyr::select(-Dia)
head(data)
print(head(daily, 20))
sum(daily%evento)
sum(daily$evento)
probs_all <- daily %>%
group_by(Site) %>%
group_modify(~ {
tibble(Probability = process_site(.x))
})
probs_all <- daily %>%
group_by(Site) %>%
group_modify(~ {
tibble(Probability = process_site(.x))
})
probs_all <- daily %>%
group_by(Site) %>%
group_modify(~ {
tibble(Probability = process_site(.x))
})
probs_df <- probs_all %>%
filter(!is.na(Probability), Probability > 0.01) %>%
arrange(desc(Probability))
data <- data %>%
filter(ANF == 83) %>%
mutate(dia = as_date(Dia)) %>%
dplyr::select(-Dia)
data <- data %>%
filter(ANF == 83) %>%
mutate(dia = as_date(Dia)) %>%
dplyr::select(-Dia)
library(tidyverse)
library(lubridate)
library(rjags)
library(readxl)
library(jsonlite)
LIMITE <- 22/24
data <- fromJSON("DISP4G.json")
data <- data %>%
filter(ANF == 83) %>%
mutate(dia = as_date(Dia)) %>%
dplyr::select(-Dia)
head(data)
daily <- data %>%
group_by(Site, dia) %>%
summarize(evento = as.integer(any(Disp <= LIMITE)), .groups = "drop") %>%
rename(periodo = dia) %>%
mutate(periodo = as_datetime(periodo))
# Bayesian Weibull model matching PyMC3 version
bayesian_weibull_model <- function(durations, events) {
model_string <- "
model {
# Priors
lambda ~ dgamma(2, 1)
rho    ~ dgamma(2, 1)
for (i in 1:N) {
# indexando log_pdf e log_S
log_pdf[i] <- log(rho) + (rho - 1) * log(durations[i])
- rho * log(lambda)
- pow(durations[i] / lambda, rho)
log_S[i]   <- -pow(durations[i] / lambda, rho)
# log-likelihood para sobrevivência ou evento
log_lik[i] <- events[i] * log_pdf[i]
+ (1 - events[i]) * log_S[i]
# ones-trick: variável auxiliar observada como 1
ones[i] ~ dbern(exp(log_lik[i]))
}
"
jags_data <- list(
durations = durations,
events = events,
N = length(durations),
ones = rep(1, length(durations))
)
inits <- function() {
list(
lambda = runif(1, 0.1, 10),
rho = runif(1, 0.1, 10)
)
}
model <- jags.model(textConnection(model_string),
data = jags_data,
inits = inits,
n.chains = 2,
quiet = TRUE)
update(model, 500)
samples <- coda.samples(model,
variable.names = c("lambda", "rho"),
n.iter = 1000)
return(samples)
}
# Processing function matching Python logic
process_site <- function(df) {
event_dates <- df %>% filter(evento == 1) %>% pull(periodo)
all_dates <- df %>% pull(periodo)
if (length(event_dates) == 0) return(NA_real_)
durations <- diff(sort(c(min(all_dates), event_dates, max(all_dates)))) %>%
as.numeric(units = "days")
events <- c(rep(1, length(event_dates)), 0)
if (length(durations) < 2) return(NA_real_)
samples <- bayesian_weibull_model(durations, events)
samples_matrix <- as.matrix(samples)
s <- tail(durations, 1)
lambda <- samples_matrix[, "lambda"]
rho <- samples_matrix[, "rho"]
mean(exp(-(s/lambda)^rho - ((s+1)/lambda)^rho))
}
# Final processing
probs_df <- daily %>%
group_by(Site) %>%
group_modify(~ {
result <- tryCatch(
list(Probability = process_site(.x)),
error = function(e) list(Probability = NA_real_))
tibble(Probability = result$Probability)
}) %>%
filter(!is.na(Probability), Probability > 0.05) %>%
arrange(desc(Probability))
probs_df
# Processing function with corrected probability calculation
process_site <- function(df) {
event_dates <- df %>% filter(evento == 1) %>% pull(periodo)
all_dates <- df %>% pull(periodo)
if (length(event_dates) == 0) return(NA_real_)
durations <- diff(sort(c(min(all_dates), event_dates, max(all_dates)))) %>%
as.numeric(units = "days")
events <- c(rep(1, length(event_dates)), 0)
if (length(durations) < 2) return(NA_real_)
samples <- bayesian_weibull_model(durations, events)
samples_matrix <- as.matrix(samples)
s <- tail(durations, 1)
lambda <- samples_matrix[, "lambda"]
rho <- samples_matrix[, "rho"]
# Corrected probability calculation
mean(exp(-(s/lambda)^rho) - exp(-((s + 1)/lambda)^rho))
}
# Final processing
probs_df <- daily %>%
group_by(Site) %>%
group_modify(~ {
result <- tryCatch(
list(Probability = process_site(.x)),
error = function(e) list(Probability = NA_real_))
tibble(Probability = result$Probability)
}) %>%
filter(!is.na(Probability), Probability > 0.05) %>%
arrange(desc(Probability))
probs_df
