}
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=0.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
# 1. Specify the model
# In this step, we give JAGS the hierarchical structure of the model,
# assigning distributions to the data (the likelihood) and parameters (priors).
# The syntax for this step is very similar to R, but there are some key differences.
library("rjags")
mod_string = " model {
for (i in 1:n) {
y[i] ~ dnorm(mu, 10.0/sig2)
}
mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom
sig2 = 1.0
} "
# 2. Set up the model
set.seed(50)
y = c(-0.2, -1.5, -5.3, 0.3, -0.8, -2.2)
n = length(y)
data_jags = list(y=y, n=n)
params = c("mu")
inits = function() {
inits = list("mu"=1.0)
} # optional (and fixed)
mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)
#3. Run the MCMC sampler
update(mod, 500) # burn-in
mod_sim = coda.samples(model=mod,
variable.names=params,
n.iter=1000)
#4. Post processing
summary(mod_sim)
library("coda")
plot(mod_sim)
library("car")  # load the 'car' package
install.packages("car")
library("car")  # load the 'car' package
library("car")  # load the 'car' package
install.packages("car")
data("PlantGrowth")
?PlantGrowth
head(PlantGrowth)
str(PlantGrowth)
boxplot(weight ~ group, data = PlantGrowth,
xlab = "Group", ylab = "Weight",
main = "Boxplot of Weight by Group",
col = c("red", "green", "blue"))
lmod = lm(weight ~ group, data = PlantGrowth)
summary(lmod)
anova(lmod)
mod_string = "model {
for (i in 1:lenght(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0, 1/1.0e6)
}
prec ~ dgamma(5/2.0, 5*1.0/2)
sig = sqrt (1 / prec)
}"
set.seed(82)
library(rjags)
data_jags = list(y = PlantGrowth$weight,
grp = as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
list("mu"=rnorm(3, 0, 100), "prec" = rgamma(1, 1, 1))
}
mod = jags.model(textConnection(mod_string),
data = data_jags,
inits = inits,
n.chains = 3,
n.adapt = 1000
)
mod_string = "model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0, 1/1.0e6)
}
prec ~ dgamma(5/2.0, 5*1.0/2)
sig = sqrt (1 / prec)
}"
set.seed(82)
library(rjags)
data_jags = list(y = PlantGrowth$weight,
grp = as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
list("mu"=rnorm(3, 0, 100), "prec" = rgamma(1, 1, 1))
}
mod = jags.model(textConnection(mod_string),
data = data_jags,
inits = inits,
n.chains = 3,
n.adapt = 1000
)
update(mod, 1000)
mod_sim = coda.samples(model = mod,
variable.names = params,
n.iter = 10000,
thin = 10
)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
mod_csim
mod_sim
mod_sim[0]
plot(mod_sim)
plot(mod_csim)
plot(mod_sim)
?coda_samples
gelman.diag(mod_sim)
autocorr.diag(mod_sim)
effectiveSize(mod_sim)
c("mu[1]", "mu[2]", "mu[3]", "sig"
pm_params(colMeans(mod_csim),
pm_params = (colMeans(mod_csim))
pm_params
yhat = pm_params[1:3][data_jags$grp]
data_jags$grp
yhat
resid = data_jags$y - yhat
resid
plot(resid ~ yhat,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
plot(resid)
plot(resid ~ yhat,
xlab = "Fitted Values", ylab = "Residuals",
main = "Residuals vs Fitted Values")
summary(mod_sim)
HPDinterval(mod_sim, prob = 0.95)
HPDinterval(mod_csim, prob = 0.95)
mod_csim[,3] > mod_csim[,1]
mean(mod_csim[,3] > mod_csim[,1])
mean(mod_csim[,3] > 1.1* mod_csim[,1])
data("PlantGrowth")
?PlantGrowth
head(PlantGrowth)
str(PlantGrowth)
boxplot(weight ~ group, data = PlantGrowth,
xlab = "Group", ylab = "Weight",
main = "Boxplot of Weight by Group",
col = c("red", "green", "blue"))
lmod = lm(weight ~ group, data = PlantGrowth)
summary(lmod)
anova(lmod)
mod_string = "model {
for (i in 1:length(y)) {
y[i] ~ dnorm(mu[grp[i]], prec)
}
for (j in 1:3) {
mu[j] ~ dnorm(0, 1/1.0e6)
}
prec ~ dgamma(5/2.0, 5*1.0/2)
sig = sqrt (1 / prec)
}"
set.seed(82)
library(rjags)
data_jags = list(y = PlantGrowth$weight,
grp = as.numeric(PlantGrowth$group))
params = c("mu", "sig")
inits = function() {
list("mu"=rnorm(3, 0, 100), "prec" = rgamma(1, 1, 1))
}
mod = jags.model(textConnection(mod_string),
data = data_jags,
inits = inits,
n.chains = 3,
n.adapt = 1000
)
update(mod, 1000)
mod_sim = coda.samples(model = mod,
variable.names = params,
n.iter = 10000,
thin = 10
)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(mod_sim)
install.packages("data.table")
install.packages("plotly")
library(SimDesign)
?Simdesign
?SimDesign
?bias
library(datasauRus)
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap(~dataset,ncol=3)
library(tidyverse)
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap(~dataset,ncol=3)
library(tidyverse)
install.packages("tidyverse")
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap(~dataset,ncol=3)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
data("datasaurus_dozen")
str(datasaurus_dozen)
head(datasaurus_dozen)
select(datasaurus_dozen)
install.packages("tidyverse")
library(tidyverse)
install.packages(rethinking)
install.packages("rethinking")
install.packages(c("coda","mvtnorm","devtools"))
q()
install.packages("rstan")
setwd("~/Documentos/COURSERA_BAYES_ESPECIALIZATION/IllinoisTechBayesianComputation/Week1/Lesson3")
# Load necessary libraries
library(rstan)
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Generate data from a true normal distribution
n <- 100  # Number of data points
true_mean <- 5
true_sd <- 2
y <- rnorm(n, mean = true_mean, sd = true_sd)
# Plot the true distribution
true_data <- data.frame(x = seq(min(y) - 1, max(y) + 1, length.out = 100))
true_data$y <- dnorm(true_data$x, mean = true_mean, sd = true_sd)
ggplot(true_data, aes(x = x, y = y)) +
geom_line(color = "blue") +
ggtitle("True Distribution") +
xlab("Value") +
ylab("Density")
# Define the Stan model
stan_model_code <- "
data {
int<lower=0> N;       // Number of data points
real y[N];            // Observations
}
parameters {
real mu;              // Mean parameter
real<lower=0> sigma;  // Standard deviation parameter
}
model {
y ~ normal(mu, sigma);  // Likelihood
}
"
# Compile the Stan model
stan_model <- stan_model(model_code = stan_model_code)
# Fit the Stan model
fit <- sampling(stan_model, data = list(N = n, y = y), iter = 1000, chains = 4, seed = 123)
print(fit)
# Fit the Stan model
fit <- sampling(stan_model, data = list(N = n, y = y), iter = 1000, chains = 4, seed = 123)
# Fit the Stan model
fit <- sampling(stan_model, data = list(N = n, y = y), iter = 1000, chains = 4, seed = 123)
print(fit)
# Extract posterior samples
posterior_samples <- extract(fit)
posterior_samples
# Calculate the mean of the posterior samples
mu_mean <- mean(posterior_samples$mu)
sigma_mean <- mean(posterior_samples$sigma)
# Plot the estimated distribution
estimated_data <- data.frame(x = seq(min(y) - 1, max(y) + 1, length.out = 100))
estimated_data$y <- dnorm(estimated_data$x, mean = mu_mean, sd = sigma_mean)
ggplot() +
geom_line(data = true_data, aes(x = x, y = y), color = "blue") +
geom_line(data = estimated_data, aes(x = x, y = y), color = "red") +
ggtitle("True Distribution (blue) vs Estimated Distribution (red)") +
xlab("Value") +
ylab("Density")
